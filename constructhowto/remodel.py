from transformers import AutoTokenizer, AutoModel
from peft import PeftModel
import torch
import os

# ================= è·¯å¾„é…ç½® =================
# 1. åŸç‰ˆæ¨¡å‹è·¯å¾„
base_model_path = r"F:\LLM\pythonProject\model"
# 2. ä½ çš„ LoRA è·¯å¾„
lora_path = r"F:\LLM\pythonProject\chatglm2_ielts_lora"
# 3. è¾“å‡ºçš„æ–°æ¨¡å‹è·¯å¾„ (ç¨‹åºä¼šè‡ªåŠ¨åˆ›å»º)
output_path = r"F:\LLM\pythonProject\chatglm2_ielts_merged"
# ===========================================

print("ğŸš€ æ­£åœ¨åŠ è½½åŸç‰ˆæ¨¡å‹ (CPUæ¨¡å¼)...")
# å…³é”®ï¼šéƒ½åœ¨ CPU ä¸ŠåŠ è½½ï¼Œé¿å…æ˜¾å­˜å’Œé‡åŒ–å†²çª
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(base_model_path, trust_remote_code=True, torch_dtype=torch.float16, device_map="cpu")

print(f"ğŸ”— æ­£åœ¨åŠ è½½ LoRA: {lora_path} ...")
model = PeftModel.from_pretrained(model, lora_path, device_map="cpu")

print("â™»ï¸ æ­£åœ¨å°† LoRA èåˆè¿›ä¸»æ¨¡å‹ (Merge)...")
# è¿™ä¸€æ­¥æŠŠ LoRA çš„æƒé‡æ°¸ä¹…åŠ åˆ°äº†åŸæ¨¡å‹é‡Œ
model = model.merge_and_unload()

print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜èåˆåçš„æ–°æ¨¡å‹åˆ°: {output_path} ...")
model.save_pretrained(output_path)
tokenizer.save_pretrained(output_path)

print("âœ… æ­å–œï¼åˆå¹¶å®Œæˆï¼")
print(f"ä»¥åè¯·ç›´æ¥åŠ è½½è¿™ä¸ªæ–°æ–‡ä»¶å¤¹ï¼š{output_path}")